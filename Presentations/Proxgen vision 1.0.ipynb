{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# `WHAT IS VISION 1.0?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## *Its a completely advanced Artificial Intelligence powered device for blind people.it helps them to safely navigate in streets and on roads.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "<!-- ![](./vision_v1_2.jpg =100x20) -->\n",
    "<img src=\"./vision_v1_2.jpg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `Device Architecture`\n",
    "\n",
    "### To fulfuill the objective of this device, it should have two basic fetures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* It should detect the object.\n",
    "* It should measure the distance of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `Process Flow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To perform the meaningfull results and to fulfill above mentioned objective system will go through 3 stages. For the purpose of this article we will call it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the sensing stage, cameras and various sensors are used to see any objects that are around the person, such as other cars, humans, bicycles, and animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "### Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "#### As a human we need eyes to see, similarly for this device various sensors would be eye.\n",
    "\n",
    "1. ***Cameras:-*** Cameras are the more similar to the our own sight, they are capturing continues pictures. Though it gives the more natural vision the things around the person, but the down side is it doesn't give much sence of the distance of the object. so for that we are going to use another sensor called Radar and Lidar. \n",
    "\n",
    "2. ***Radar:-*** Radar has been traditionally used to detecting moving object like aricraft and weather formations. it works by transmitting radio waves in bursts or pulses. Once those waves hit an object, they bounce right back to the sensor, it's a perfect column for cameras since cameras can not detect objects in night, but radar and lidar can can because they works by transmitting waves it doesn't matter day or night. The drawback of radar is that the technology is currently limited in its accuracy.\n",
    "\n",
    "3. ***LiDAR***:- LiDAR stands for Light detection and ranging. It works by sending out beams of light and then calculating how long it takes for the light to hit an object and reflect back to LiDAR scanner. Big advantage of the LiDAR is their accuarcy.\n",
    "\n",
    "![How ](https://thumbs.gfycat.com/ComplexThirstyClownanemonefish-max-1mb.gif)\n",
    "![](https://media1.giphy.com/media/3oEduYqb4Ty6dSReKc/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This stage is like the brain of the device, In this stage major processing will take part. We gather the information from the sensor and by processing it we well get to know what is going on in the surrounding. For example\n",
    "\n",
    "* What are the objects.\n",
    "* where are they moving.\n",
    "* Are we going in right direction or not.\n",
    "\n",
    "![](https://miro.medium.com/max/1600/1*8gmgaAkFdI-9OHY5cA93xQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3. Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Till now computer vision has been done it's part, with this we will move on to our next stage, now time is for get into real action.\n",
    "Now we have information about surroundings, so our device will tell the person in the conversational manner who is coming, is there any obstacle in the path, how far the car is, what is the speed of it, what is the color of traffic signal, and most importantly in which direction he should move to reach the destination and how much time it will take."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
